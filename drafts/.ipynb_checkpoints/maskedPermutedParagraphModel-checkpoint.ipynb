{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26131c9a-df2b-4145-b94e-0a17ab6ec0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MPNetForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-mpnet-base-v2 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import MPNetForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = MPNetForSequenceClassification.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\", num_labels=3)  # Adjust num_labels based on your task\n",
    "#model = SentenceTransformer('local_models/all-mpnet-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d00d0b55-48ee-413f-baaf-0359e74e38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MPNetTokenizer # Specifyes the tokenizer speficic for model choice\n",
    "from datasets import load_dataset, Dataset # We need these to be able to convert our dataset from pandas dataframe format to a trainable format mainly huggingface format\n",
    "import pandas as pd # will be used to load the fine tuning and training fdataset from the environment\n",
    "\n",
    "# Load tokenizer to split the paragraph word by word and assign value or meassure for each word\n",
    "tokenizer = MPNetTokenizer.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Load your dataset which has a 'premise', 'hypothesis, and 'label' columns\n",
    "# Here the premise is a paragrahp from project reports over time which are annual reports and other reports.\n",
    "#\n",
    "data = pd.read_csv(\"sample_data.csv\")  # Ensure your CSV has 'text' and 'label' columns\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "dataset = Dataset.from_pandas(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7df00681-cfcc-4712-ac4b-d992426fe93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa624db7-a0a8-4ae3-bd1b-6aae36acbdca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b970c260914a1281fcd8badafa3bbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[490, 512, 590, 572, 478, 556, 201, 573, 454, 611, 351, 136, 79, 735, 709, 155, 312, 724, 650, 640, 580, 461, 613, 690, 540, 546, 626, 760, 541, 593, 619, 58, 53, 391, 717, 188, 610, 879, 729, 731, 732, 712, 883, 694, 827, 821, 894, 906, 905, 839, 841, 29, 116, 839, 577, 121, 742, 762, 744, 671, 616, 304, 612, 585, 371, 608, 621, 329, 469, 695, 387, 603, 515, 499, 558, 651, 659, 608, 640, 253, 605, 579, 584, 654, 550, 531, 502, 645, 669, 309, 384, 379, 384, 780, 621, 780, 119, 225, 235, 960]\n",
      "[48, 34, 46, 17, 31, 17, 14, 32, 24, 34, 34, 45, 27, 25, 18, 31, 28, 15, 17, 34, 34, 40, 70, 28, 11, 32, 22, 51, 43, 61, 70, 26, 37, 46, 37, 61, 67, 28, 69, 64, 33, 36, 34, 48, 35, 67, 34, 33, 63, 8, 54, 15, 34, 31, 77, 32, 28, 43, 13, 49, 29, 88, 48, 26, 35, 16, 32, 42, 41, 15, 28, 115, 35, 20, 41, 31, 21, 70, 28, 62, 42, 41, 40, 42, 72, 38, 29, 42, 33, 15, 30, 34, 37, 18, 33, 28, 30, 12, 70, 29]\n"
     ]
    }
   ],
   "source": [
    "#To ensure thata we are aware of the trancation in the input paragraph\n",
    "#we check the lenght of the 'premise' and 'hypothesis' and identify potential candidates\n",
    "#for truncation we then Inspect the Length of Input Sequences which we set as premise and hypothesis:\n",
    "#Before tokenization, you can check the length of your premise and hypothesis pairs to identify potential candidates for truncation\n",
    "\n",
    "def check_length(example):\n",
    "  premise_len = [len(tokenizer.tokenize(premise)) for premise in example['premise']]\n",
    "  hypothesis_len = [len(tokenizer.tokenize(hypothesis)) for hypothesis in example['hypothesis']]\n",
    "  return {'premise_len': premise_len, 'hypothesis_len': hypothesis_len}\n",
    "\n",
    "lengths = dataset.map(check_length, batched=True)\n",
    "print(lengths['premise_len'])\n",
    "print(lengths['hypothesis_len'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32df21eb-6bcd-415d-9fb6-6efa8b35e92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can Reduce Sequence Length (Optional): If you find that too many tokens are being truncated,\n",
    "#you can reduce the max_length argument in the tokenizer to prevent excessive truncation.\n",
    "###########################################################################################\n",
    "#Return Overflowing Tokens If you want to see the tokens that are truncated and removed,\n",
    "#you can set the return_overflowing_tokens=True argument in the tokenizer call.\n",
    "#This way, even the tokens that are discarded due to truncation will be returned and can be inspected:\n",
    "#############################################################################################\n",
    "#Change Truncation Strategy Instead of using 'longest_first' for truncation\n",
    "# (which discards tokens without providing them), you can switch to a truncation\n",
    "# strategy that better suits your needs, such as truncating one side only or customizing the behavior:\n",
    "#    'only_first': This truncates tokens from the first sequence (the premise) only.\n",
    "#    'only_second': This truncates tokens from the second sequence (the hypothesis) only.\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['premise'],\n",
    "        examples['hypothesis'],\n",
    "        truncation='only_first',        #'only_first',  # This will only truncate the premise since we saw that the length of the hypothesis is short\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,  # Reduce max_length if needed\n",
    "        return_overflowing_tokens=False,\n",
    "        return_special_tokens_mask=True  # Optional: helps in understanding which tokens were removed\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a361458-76c5-4298-82db-ac5401115a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685f9524fa834ce1987abd044df08413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This part is to tokenize both the 'premise' and the 'hypothesis'\n",
    "#def preprocess_function(examples):\n",
    "#    return tokenizer(examples['premise'], examples['hypothesis'], truncation=True, padding=\"max_length\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9a65ca9-2c77-4528-9173-5e347120834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0:\n",
      "Tokens: ['<s>', 'the', 'international', 'potato', 'industry', 'information', 'in', 'order', 'to', 'educate', 'us', 'and', 'canadian', 'consumers', '.', 'both', 'sides', 'of', 'the', 'gm', '##o', 'issue', 'will', 'raise', 'and', 'spend', 'money', 'trying', 'to', 'influence', 'societal', 'acceptance', 'of', 'gm', '##o', 'food', '.', 'the', 'leading', 'anti', '-', 'gm', '##o', 'organization', 'is', 'green', '##pe', '##ace', '(', 'website', 'http', ':', '/', '/', 'www', '.', 'green', '##pe', '##ace', '.', 'org', ')', '.', 'if', 'the', 'societal', 'acceptance', 'pattern', 'for', 'gm', '##os', 'is', 'similar', 'to', 'that', 'of', 'microwave', 'oven', '##s', ',', 'gm', '##o', 'potatoes', 'will', 'eventually', 'be', 'accepted', '.', 'gm', '##o', 'acceptance', 'is', 'slowed', 'by', 'concerns', 'about', 'health', 'and', 'the', 'environment', ',', 'which', 'are', 'similar', 'forces', 'to', 'the', 'health', 'and', 'cost', 'concerns', 'that', 'slowed', 'microwave', 'oven', 'acceptance', '.', 'consumer', 'convenience', 'was', 'a', 'powerful', 'accelerating', 'force', 'in', 'microwave', 'oven', 'acceptance', '.', 'the', 'gm', '##o', 'industry', 'does', 'not', 'yet', 'have', 'such', 'a', 'consumer', '-', 'acceptance', 'accelerator', '.', 'product', 'development', 'has', 'focused', 'on', 'attributes', 'that', 'are', 'attractive', 'to', 'producers', 'rather', 'than', 'consumers', '.', 'if', 'the', 'industry', 'develops', 'products', 'with', 'a', 'powerful', 'consumer', 'pull', ',', 'it', 'might', 'com', '##press', 'the', 'societal', 'acceptance', 'timeline', '.', 'seed', 'technology', 'the', 'global', 'potato', 'industry', 'faces', 'a', 'planting', 'material', 'challenge', '.', 'the', 'traditional', 'use', 'of', 'seed', 'tube', '##rs', 'carries', 'the', 'problems', 'of', 'seed', '-', 'borne', 'disease', ',', 'low', 'multi', '##pl', '##i', '-', 'cat', '##ion', 'rates', 'and', 'high', 'costs', 'of', 'storage', 'and', 'transport', '.', 'true', 'potato', 'seed', '(', 't', '##ps', ')', 'can', 'solve', 'those', 'problems', 'but', 'creates', 'other', 'problems', 'including', 'genetic', 'variation', ',', 'slow', 'plant', 'development', 'and', 'high', 'labour', 'costs', '.', 'both', 'types', 'of', 'seed', 'systems', 'work', 'and', 'are', 'used', 'around', 'the', 'world', ',', 'but', 'costs', 'and', 'risks', 'are', 'high', '.', 'new', 'technology', ',', 'however', ',', 'is', 'improving', 'potato', '-', 'planting', 'material', 'and', 'red', '##uc', '-', 'ing', 'some', 'of', 'the', 'costs', 'and', 'risks', '.', 'the', 'production', 'of', 'basic', 'seed', 'and', 'its', 'multiplication', 'has', 'benefited', 'from', 'some', 'major', 'technological', 'advances', '.', 'the', 'old', 'system', 'of', 'finding', 'healthy', ',', 'disease', '-', 'free', 'potato', 'plants', 'in', 'the', 'field', 'and', 'rep', '##rod', '##uc', '##ing', 'them', 'as', 'cl', '##onal', 'se', '##le', '##c', '-', 'ti', '##ons', 'in', 'subsequent', 'field', 'generations', 'has', 'become', 'outdated', '.', 'the', 'new', 'system', 'involves', 'the', 'production', 'of', 'pre', '-', 'basic', 'seed', 'in', 'artificial', ',', 'sterile', 'conditions', 'in', 'laboratories', '.', 'known', 'as', 'ain', 'vitro', '##a', 'systems', ',', 'the', 'process', 'uses', 'plant', 'parts', 'to', 'produce', 'whole', 'plants', 'that', 'are', 'free', 'from', 'disease', '.', 'new', 'technology', 'will', 'focus', 'on', 'reducing', 'costs', 'while', 'maintaining', 'quality', 'during', 'multiplication', '.', 'one', 'important', 'aspect', 'of', 'the', 'seed', 'tube', '##r', 'production', 'system', 'is', 'the', 'reduction', 'of', 'time', 'from', 'laboratory', 'to', 'commercial', 'potato', 'grow', '##er', '.', 'the', 'trend', 'is', 'toward', 'fewer', 'generations', 'of', 'seed', 'potato', 'multiplication', 'in', 'the', 'field', '.', 'carrying', 'the', 'trend', 'to', 'the', 'limit', ',', 'eventually', 'commercial', 'potato', 'grow', '##ers', 'might', 'plant', 'ain', 'vitro', '##a', 'material', 'that', 'has', 'never', 'been', 'in', '</s>', '</s>', '6', '.', 'a', 'by', '203', '##0', ',', 'expand', 'international', 'cooperation', 'and', 'capacity', '-', 'building', 'support', 'to', 'developing', 'countries', 'in', 'water', '-', 'and', 'sanitation', '-', 'related', 'activities', 'and', 'programmes', ',', 'including', 'water', 'harvesting', ',', 'des', '##ali', '##nation', ',', 'water', 'efficiency', ',', 'wastewater', 'treatment', ',', 'recycling', 'and', 're', '##use', 'technologies', '</s>']\n",
      "Sample 1:\n",
      "Tokens: ['<s>', 'technology', 'issues', 'seed', 'tube', '##rs', ',', 'especially', 'in', 'developing', 'countries', 'where', 'high', '-', 'quality', 'seed', 'tube', '##rs', 'are', 'difficult', 'to', 'produce', 'domestically', 'and', 'expensive', 'to', 'import', '.', 'ad', '##van', '-', 'tag', '##es', 'of', 't', '##ps', 'include', 'the', 'fact', 'that', 'most', 'diseases', 'are', 'not', 'transmitted', 'in', 't', '##ps', ',', 'low', 'transport', 'costs', 'owing', 'to', 'small', 'seed', 'size', 'and', 'a', 'storage', 'life', 'of', 'two', 'years', 'at', 'room', 'temperatures', '.', 't', '##ps', 'is', 'labour', '-', 'intensive', 'because', 'it', 'requires', 'hand', 'poll', '##ination', ',', 'but', 'that', 'may', 'not', 'be', 'a', 'barrier', 'in', 'places', 'where', 'labour', 'is', 'cheap', '.', 't', '##ps', 'use', 'is', 'increasing', 'in', 'scattered', 'locations', 'around', 'the', 'globe', 'including', 'bangladesh', ',', 'china', ',', 'egypt', ',', 'india', ',', 'nicaragua', ',', 'peru', 'and', 'vietnam', '.', 'one', 'problem', 'with', 'the', 'use', 'of', 't', '##ps', 'is', 'that', 'the', 'commercial', 'potato', 'crops', 'produced', 'from', 't', '##ps', 'do', 'not', 'have', 'the', 'consistent', 'quality', 'attributes', 'demanded', 'by', 'potato', 'processors', '.', 'several', 'large', 'firms', 'are', 'active', 'in', 'global', 'seed', 'potato', 'markets', '.', 'some', 'are', 'in', 'the', 'import', '##ae', '##x', '##port', 'business', ',', 'some', 'own', 'varieties', 'and', 'some', 'market', 'seed', 'production', 'technology', '.', 'one', 'firm', ',', 'tech', '##nic', '##o', ',', 'sells', 'entire', 'seed', 'chain', 'systems', 'based', 'on', 'patented', 'techniques', 'for', 'mini', '##tub', '##er', 'production', 'and', 'early', '-', 'generation', 'seed', 'multiplication', '.', 'tech', '##nic', '##o', 'ad', '##vert', '##ises', 'that', 'it', 'can', 'produce', 'seed', 'potatoes', 'in', 'as', 'little', 'as', 'two', 'years', 'from', 'laboratory', 'to', 'commercial', 'grow', '##er', '.', 'in', '2000', 'the', 'company', 'had', 'production', 'operations', 'in', 'the', 'united', 'states', ',', 'mexico', ',', 'australia', ',', 'china', 'and', 'india', '.', 'tech', '##nic', '##o', 'is', 'example', 'of', 'the', 'benefits', 'of', 'private', 'property', 'rights', 'in', 'the', 'seed', 'potato', 'industry', '.', 'without', 'legal', 'protection', 'of', 'its', 'intellectual', 'property', ',', 'there', 'would', 'have', 'been', 'little', 'or', 'no', 'economic', 'incentive', 'for', 'tech', '-', 'nico', 'to', 'develop', 'a', 'technology', 'that', 'can', 'benefit', 'potato', 'industries', 'in', 'both', 'dev', '##el', '-', 'op', '##ed', 'and', 'developing', 'countries', '.', 'plant', 'property', 'rights', 'intellectual', 'property', 'rights', 'laws', 'in', 'some', 'countries', 'have', 'created', 'op', '##port', '##u', '-', 'ni', '##ties', 'to', 'own', 'potato', 'varieties', 'in', 'general', ',', 'not', 'just', 'those', 'developed', 'with', 'gm', '##o', 'technology', '.', 'for', 'example', ',', 'the', 'netherlands', 'national', 'seed', 'law', 'of', '1967', 'puts', 'potato', 'varieties', 'into', 'two', 'categories', ':', 'licensed', 'varieties', 'and', 'public', 'varieties', '.', 'grow', '##ers', 'can', 'plant', 'public', 'varieties', ',', 'also', 'known', 'as', 'af', '##ree', '##ava', '##rie', '##ties', ',', 'without', 'paying', 'fees', '.', 'grow', '##ers', 'who', 'plant', 'licensed', 'varieties', 'must', 'pay', 'royalty', 'fees', 'to', 'the', 'owners', 'of', 'the', 'varieties', '.', 'the', 'licence', 'system', 'provides', 'economic', 'inc', '##en', '-', 'ti', '##ves', 'for', 'plant', 'breeders', 'to', 'develop', 'new', 'varieties', 'and', 'be', 'compensated', 'for', 'their', 'efforts', ',', 'expense', 'and', 'risk', '.', 'the', 'government', ',', 'in', 'effect', ',', 'gives', 'the', 'owner', 'a', 'monopoly', ',', 'but', 'only', 'for', 'a', 'fixed', 'amount', 'of', 'time', '.', 'thirty', 'years', 'after', 'the', 'first', 'licensing', 'in', 'the', 'netherlands', ',', 'a', 'variety', 'becomes', 'public', ',', 'available', 'to', 'all', '</s>', '</s>', '16', '.', 'a', 'strengthen', 'relevant', 'national', 'institutions', ',', 'including', 'through', 'international', 'cooperation', ',', 'for', 'building', 'capacity', 'at', 'all', 'levels', ',', 'in', 'particular', 'in', 'developing', 'countries', ',', 'to', 'prevent', 'violence', 'and', 'combat', 'terrorism', 'and', 'crime', '</s>']\n",
      "Sample 2:\n",
      "Tokens: ['<s>', 'the', 'international', 'potato', 'industry', 'opportunities', 'to', 'protect', 'intellectual', 'property', 'rights', 'and', 'own', 'potato', 'var', '##i', '-', 'et', '##ies', 'were', 'a', 'factor', 'in', 'the', 'development', 'of', 'a', 'private', 'breeding', 'industry', 'in', 'the', 'netherlands', '.', 'the', 'powerful', 'economic', 'incentives', 'to', 'develop', 'new', 'varieties', 'in', 'the', 'netherlands', 'did', 'not', 'exist', 'in', 'other', 'countries', ',', 'including', 'the', 'united', 'states', ',', 'until', 'decades', 'after', 'the', 'dutch', 'government', 'allowed', 'variety', 'ownership', '.', 'st', '##ru', '##ik', 'and', 'wi', '##ers', '##ema', 'point', 'out', 'that', 'plant', 'property', 'rights', 'also', 'provide', 'benefits', 'to', 'those', 'who', 'do', 'not', 'own', 'varieties', '.', 'a', 'large', 'assortment', 'of', 'dutch', 'var', '##i', '-', 'et', '##ies', 'gives', 'all', 'dutch', 'seed', 'grow', '##ers', 'opportunities', 'in', 'the', 'international', 'market', 'place', 'that', 'seed', 'grow', '##ers', 'in', 'other', 'countries', 'lack', '.', 'the', 'variety', 'ownership', 'system', 'also', 'provides', 'price', 'stability', '.', 'private', 'variety', 'prices', 'in', 'the', 'netherlands', 'are', 'on', 'average', 'higher', 'than', 'public', 'varieties', ',', 'but', 'they', 'are', 'less', 'volatile', '.', 'since', 'variety', 'owners', 'can', 'control', 'planting', '##s', 'they', 'can', 'better', 'match', 'production', 'with', 'market', 'demand', '.', 'planting', '##s', 'of', 'public', 'varieties', 'are', 'not', 'controlled', 'and', 'vary', 'in', 'a', 'boom', '-', 'or', '-', 'bust', 'cycle', 'of', 'high', 'prices', 'followed', 'by', 'low', 'prices', 'that', 'is', 'common', 'in', 'the', 'potato', 'industry', '.', 'plant', 'property', 'rights', 'can', 'help', 'change', 'potato', 'products', 'from', 'com', '-', 'mod', '##ities', 'to', 'value', '-', 'added', 'branded', 'products', '.', 'as', 'we', 'have', 'seen', ',', 'a', 'problem', 'for', 'grow', '##ers', 'is', 'that', 'long', '-', 'run', 'commodity', 'prices', 'trend', 'downward', '.', 'if', 'the', 'variety', 'that', 'is', 'owned', 'is', 'differentiated', 'from', 'the', 'public', 'varieties', 'and', 'supply', 'can', 'be', 'controlled', ',', 'producers', 'can', 'gain', 'higher', 'prices', '.', 'if', 'the', 'producers', 'themselves', 'own', 'the', 'varieties', 'they', 'can', 'control', 'who', 'grows', 'the', 'variety', 'and', 'prevent', 'the', 'market', 'from', 'being', 'flooded', '.', 'the', 'issue', 'of', 'who', 'owns', 'the', 'plant', 'property', 'rights', 'determines', 'who', 'col', '-', 'le', '##cts', 'the', 'returns', 'to', 'ownership', '.', 'many', 'of', 'the', 'links', 'on', 'the', 'marketing', 'chain', 'have', 'incentives', 'to', 'own', 'varieties', '.', 'at', 'the', 'grow', '##er', 'end', 'of', 'the', 'chain', ',', 'both', 'ind', '##i', '-', 'vi', '##du', '##als', 'and', 'groups', 'of', 'grow', '##ers', 'own', 'varieties', '.', 'ag', '##ric', '##o', ',', 'a', 'grow', '##er', 'co', '-', 'operative', 'with', 'headquarters', 'in', 'the', 'netherlands', ',', 'employs', 'a', 'plant', 'breeding', 'staff', 'to', 'develop', 'new', 'potato', 'varieties', '.', 'ag', '##ric', '##o', 'is', 'active', 'in', 'international', 'trade', 'so', 'it', 'also', 'attempts', 'to', 'secure', 'legal', 'property', 'rights', 'for', 'its', 'varieties', 'in', 'each', 'country', 'in', 'which', 'it', 'operates', '.', 'individual', 'farming', 'operations', 'also', 'acquire', 'variety', 'prop', '-', 'er', '##ty', 'rights', 'either', 'through', 'purchasing', 'them', ',', 'hiring', 'plant', 'breeders', 'or', 'di', '##s', '-', 'covering', 'mu', '##tated', 'plants', 'in', 'their', 'fields', ',', 'which', 'they', 'can', 'patent', 'and', 'trademark', '.', 'one', 'example', 'of', 'individual', 'grow', '##er', 'ownership', 'is', 'the', 'mcc', '##ull', '##ough', 'family', 'in', 'washington', 'that', 'owns', 'the', 'hi', '##lite', 'russ', '##et', 'variety', '.', 'at', 'the', '</s>', '</s>', '2', '.', 'a', 'increase', 'investment', ',', 'including', 'through', 'enhanced', 'international', 'cooperation', ',', 'in', 'rural', 'infrastructure', ',', 'agricultural', 'research', 'and', 'extension', 'services', ',', 'technology', 'development', 'and', 'plant', 'and', 'livestock', 'gene', 'banks', 'in', 'order', 'to', 'enhance', 'agricultural', 'productive', 'capacity', 'in', 'developing', 'countries', ',', 'in', 'particular', 'least', 'developed', 'countries', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Inspect overflowing tokens\n",
    "# You can then check whether any tokens were truncated:\n",
    "# Inspect a sample for truncation\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(\"Tokens:\", tokenizer.convert_ids_to_tokens(tokenized_datasets[i]['input_ids']))\n",
    "    if 'overflowing_tokens' in tokenized_datasets[i]:\n",
    "        print(\"Overflowing tokens:\", tokenizer.convert_ids_to_tokens(tokenized_datasets[i]['overflowing_tokens']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a48f2d3-55c9-42ad-8b36-19712ce296d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenized_datasets[0]['premise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bc11f85-564c-4e4b-90dc-06e818315480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'special_tokens_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "{'premise': Value(dtype='string', id=None), 'hypothesis': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'special_tokens_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets without stratification\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "# Check the train and validation datasets to ensure they contain the labels\n",
    "print(train_dataset.features)\n",
    "print(eval_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d24853-105e-40c6-a484-2d260450ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    logging_dir=\"./logs\",\n",
    "    #no_cuda=True     This parameter has been deprecated we use the below parameter instead\n",
    "    use_cpu=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f663734-807a-42d5-b2c8-a0c9325dad1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import DataCollatorForSequenceClassification, Trainer\n",
    "\n",
    "#data_collator = DataCollatorForSequenceClassification(tokenizer=tokenizer, model=model)\n",
    "\n",
    "from transformers import DataCollatorWithPadding, Trainer\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    #train_dataset=tokenized_datasets,\n",
    "    #eval_dataset=tokenized_datasets,  # Typically you would split into train and validation datasets\n",
    "    train_dataset=train_dataset,  # Use the training dataset\n",
    "    eval_dataset=eval_dataset,    # Use the validation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "70c26a9f-77c0-43e0-8174-3d4bdcbdcbf5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AdamW' object has no attribute 'train'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/transformers/trainer.py:2388\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   2387\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 2388\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2391\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2392\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2393\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2394\u001b[0m ):\n\u001b[1;32m   2395\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2396\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/transformers/trainer.py:3477\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3475\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mtrain):\n\u001b[0;32m-> 3477\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3479\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_inputs(inputs)\n\u001b[1;32m   3480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sagemaker_mp_enabled():\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/accelerate/optimizer.py:128\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03m    Sets the optimizer to \"train\" mode. Useful for optimizers like `schedule_free`\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AdamW' object has no attribute 'train'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f090b81-d9a1-40d9-9a12-98d7cde5f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='weighted')\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4684d-bdfb-4249-8bfe-2d13c91f4055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/60 : < :, Epoch 0.05/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        # Remove the self.optimizer.train() call\n",
    "        model.train()  # Ensure model is in training mode\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        # Forward pass\n",
    "        loss = self.compute_loss(model, inputs)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss.detach()\n",
    "\n",
    "# Now use CustomTrainer instead of Trainer\n",
    "custom_trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "custom_trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bf3b62ec-2126-4d62-9c9d-2326205c67b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "local_model/all-mpnet-base-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/local_model/all-mpnet-base-v2/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1257\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1347\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1752\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[1;32m   1753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1674\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1675\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1680\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1681\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1682\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/file_download.py:400\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 400\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/huggingface_hub/utils/_errors.py:352\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    344\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    345\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    346\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    351\u001b[0m     )\n\u001b[0;32m--> 352\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-67189b4c-3f6baa5c716c6a58612c9542;d36970ff-2534-46bf-9ccf-6befc67100f3)\n\nRepository Not Found for url: https://huggingface.co/local_model/all-mpnet-base-v2/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_pandas(data)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mMPNetTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocal_model/all-mpnet-base-v2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Preprocessing function\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_function\u001b[39m(examples):\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2127\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer_file\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m vocab_files:\n\u001b[1;32m   2125\u001b[0m     \u001b[38;5;66;03m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[39;00m\n\u001b[1;32m   2126\u001b[0m     fast_tokenizer_file \u001b[38;5;241m=\u001b[39m FULL_TOKENIZER_FILE\n\u001b[0;32m-> 2127\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2130\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2137\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2138\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2139\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2140\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2142\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2144\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/CIPotatoe/CIP_env/lib/python3.11/site-packages/transformers/utils/hub.py:426\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    436\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: local_model/all-mpnet-base-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import MPNetForSequenceClassification, MPNetTokenizer, AdamW\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "#dataset = load_dataset('your_dataset_name')  # Replace with your actual dataset\n",
    "\n",
    "data = pd.read_csv(\"sample_data.csv\")  # Ensure your CSV has 'text' and 'label' columns\n",
    "\n",
    "# Convert to Hugging Face dataset format\n",
    "dataset = Dataset.from_pandas(data)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = MPNetTokenizer.from_pretrained('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['premise'],  # Replace with your actual input field\n",
    "        examples['hypothesis'],  # Replace with your actual input field\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors='pt'  # Ensure it returns tensors\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Convert the dataset to PyTorch tensors\n",
    "def convert_to_tensors(examples):\n",
    "    return {\n",
    "        'input_ids': torch.tensor(examples['input_ids']),\n",
    "        'attention_mask': torch.tensor(examples['attention_mask']),\n",
    "        'labels': torch.tensor(examples['label'])  # Adjust this based on your dataset\n",
    "    }\n",
    "\n",
    "# Apply conversion to tensors\n",
    "tokenized_dataset = tokenized_dataset.map(convert_to_tensors, batched=True)\n",
    "\n",
    "# Prepare DataLoader\n",
    "train_loader = DataLoader(tokenized_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Load model\n",
    "#model = MPNetForSequenceClassification.from_pretrained('local_models/all-mpnet-base-v2', num_labels=3)  # Adjust num_labels based on your classification task\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Move model to appropriate device (GPU or CPU)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Custom training loop\n",
    "model.train()  # Set the model to training mode\n",
    "\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()  # Clear previous gradients\n",
    "        \n",
    "        # Debugging: Print batch contents to check their types\n",
    "        print(\"Batch Contents:\", batch)\n",
    "        \n",
    "        # Move tensors to the appropriate device (GPU or CPU)\n",
    "        inputs = {\n",
    "            'input_ids': torch.stack(batch['input_ids']).to(device),  # Stack the input ids\n",
    "            'attention_mask': torch.stack(batch['attention_mask']).to(device),  # Stack the attention masks\n",
    "            'labels': torch.tensor(batch['labels']).to(device),  # Adjust this based on your dataset\n",
    "        }\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)  # Forward pass\n",
    "        loss = outputs.loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")  # Print loss for monitoring\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained('new_local_model')  # Replace with your save directory\n",
    "tokenizer.save_pretrained('new_local_model_tokenizer')  # Save tokenizer too\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2533221f-c729-44df-8e55-57cbaf753acb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4436100e-00a1-4d30-84b4-5680b21a3384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
