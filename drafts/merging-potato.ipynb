{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":215405623,"sourceType":"kernelVersion"},{"sourceId":120002,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":100933,"modelId":121027}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n%pip install -U bitsandbytes\n%pip install transformers==4.44.2\n%pip install -U accelerate\n%pip install -U peft\n%pip install -U trl==0.12.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:44:31.334597Z","iopub.execute_input":"2024-12-30T22:44:31.334871Z","iopub.status.idle":"2024-12-30T22:45:29.859547Z","shell.execute_reply.started":"2024-12-30T22:44:31.334845Z","shell.execute_reply":"2024-12-30T22:45:29.858366Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Install dependencies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:46:57.427638Z","iopub.execute_input":"2024-12-30T22:46:57.428058Z","iopub.status.idle":"2024-12-30T22:46:57.802561Z","shell.execute_reply.started":"2024-12-30T22:46:57.428023Z","shell.execute_reply":"2024-12-30T22:46:57.801760Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llama-finetune-potato/data.json\n/kaggle/input/llama-finetune-potato/__results__.html\n/kaggle/input/llama-finetune-potato/__notebook__.ipynb\n/kaggle/input/llama-finetune-potato/__output__.json\n/kaggle/input/llama-finetune-potato/custom.css\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/adapter_model.safetensors\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/adapter_config.json\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/README.md\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/tokenizer.json\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/tokenizer_config.json\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/pytorch_model.bin\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/special_tokens_map.json\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/adapter_model.safetensors\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/trainer_state.json\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/training_args.bin\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/adapter_config.json\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/README.md\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/tokenizer.json\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/tokenizer_config.json\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/scheduler.pt\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/special_tokens_map.json\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/optimizer.pt\n/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/checkpoint-489/rng_state.pth\n/kaggle/input/llama-finetune-potato/wandb/run-20241230_103729-6u7tkw7u/run-6u7tkw7u.wandb\n/kaggle/input/llama-finetune-potato/wandb/run-20241230_103729-6u7tkw7u/logs/debug.log\n/kaggle/input/llama-finetune-potato/wandb/run-20241230_103729-6u7tkw7u/logs/debug-internal.log\n/kaggle/input/llama-finetune-potato/wandb/run-20241230_103729-6u7tkw7u/files/wandb-summary.json\n/kaggle/input/llama-finetune-potato/wandb/run-20241230_103729-6u7tkw7u/files/config.yaml\n/kaggle/input/llama-finetune-potato/wandb/run-20241230_103729-6u7tkw7u/files/output.log\n/kaggle/input/llama-finetune-potato/wandb/run-20241230_103729-6u7tkw7u/files/requirements.txt\n/kaggle/input/llama-finetune-potato/wandb/run-20241230_103729-6u7tkw7u/files/wandb-metadata.json\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/config.json\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/README.md\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/USE_POLICY.md\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/tokenizer.json\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/tokenizer_config.json\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/LICENSE.txt\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/model.safetensors\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/special_tokens_map.json\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/.gitattributes\n/kaggle/input/llama-3.2/transformers/1b-instruct/1/generation_config.json\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:47:12.093335Z","iopub.execute_input":"2024-12-30T22:47:12.093844Z","iopub.status.idle":"2024-12-30T22:47:12.680652Z","shell.execute_reply.started":"2024-12-30T22:47:12.093810Z","shell.execute_reply":"2024-12-30T22:47:12.679970Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Model\nbase_model_url = \"/kaggle/input/llama-3.2/transformers/1b-instruct/1/\"\nnew_model_url = \"/kaggle/input/llama-finetune-potato/llama-finetunes-potato-wizard_v59/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:47:22.239469Z","iopub.execute_input":"2024-12-30T22:47:22.240122Z","iopub.status.idle":"2024-12-30T22:47:22.243988Z","shell.execute_reply.started":"2024-12-30T22:47:22.240090Z","shell.execute_reply":"2024-12-30T22:47:22.243013Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, pipeline\nfrom peft import PeftModel\nimport torch\nfrom trl import setup_chat_format\n# Reload tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(base_model_url)\n\nbase_model_reload= AutoModelForCausalLM.from_pretrained(\n    base_model_url,\n    low_cpu_mem_usage=True,\n    return_dict=True,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:47:29.526982Z","iopub.execute_input":"2024-12-30T22:47:29.527590Z","iopub.status.idle":"2024-12-30T22:47:59.205596Z","shell.execute_reply.started":"2024-12-30T22:47:29.527557Z","shell.execute_reply":"2024-12-30T22:47:59.204641Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Merge adapter with base model\n#base_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n#model = PeftModel.from_pretrained(base_model_reload, new_model_url)\n\n#model = model.merge_and_unload()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:48:49.906638Z","iopub.execute_input":"2024-12-30T22:48:49.907347Z","iopub.status.idle":"2024-12-30T22:48:49.911509Z","shell.execute_reply.started":"2024-12-30T22:48:49.907313Z","shell.execute_reply":"2024-12-30T22:48:49.910588Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Reset chat_template to None\ntokenizer.chat_template = None\n\n# Merge adapter with base model\nbase_model_reload, tokenizer = setup_chat_format(base_model_reload, tokenizer)\n\n# Load and merge the adapter\nmodel = PeftModel.from_pretrained(base_model_reload, new_model_url)\nmodel = model.merge_and_unload()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:48:58.786617Z","iopub.execute_input":"2024-12-30T22:48:58.787419Z","iopub.status.idle":"2024-12-30T22:48:59.998978Z","shell.execute_reply.started":"2024-12-30T22:48:58.787385Z","shell.execute_reply":"2024-12-30T22:48:59.998057Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:599: UserWarning: Found missing adapter keys while loading the checkpoint: ['base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight']\n  warnings.warn(f\"Found missing adapter keys while loading the checkpoint: {missing_keys}\")\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"instruction = \"\"\"You are a top-rated plant breeder and agronomy service agent \nnamed Alison. Give optimum potato trait combinations for potato varieties amid \nclimate change and disease pressure. Be polite to farmers and breeders and answer \nall their questions.\"\"\"\n\nmessages = [{\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"Tell me about a potato variety called shangi\"}]\n\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    \ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T23:10:27.983236Z","iopub.execute_input":"2024-12-30T23:10:27.983677Z","iopub.status.idle":"2024-12-30T23:10:30.859856Z","shell.execute_reply.started":"2024-12-30T23:10:27.983647Z","shell.execute_reply":"2024-12-30T23:10:30.859037Z"}},"outputs":[{"name":"stdout","text":"\nI'm happy to help you with your question about Shangi Sot, a popular potato variety.\n\nShangi Sot is a popular potato variety that originated in the 1960s in Japan. It is known for its high yield, disease resistance, and adaptability to various climates. Here are some optimum trait combinations for Shangi Sot:\n\n**Optimum Trait Combinations:**\n\n1. **Yield:** Shangi Sot is known for its high yield, with an average yield of 5-7 kg (11-15 lbs) per plant. Optimum trait combination: High Yield (H) x Disease Resistance (D) x Adaptability (A)\n2. **Disease Resistance:** Shangi Sot has excellent disease\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"new_model = \"potato_wizard_v59\"\n\nmodel.save_pretrained(new_model)\ntokenizer.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:50:36.674751Z","iopub.execute_input":"2024-12-30T22:50:36.675548Z","iopub.status.idle":"2024-12-30T22:50:43.790600Z","shell.execute_reply.started":"2024-12-30T22:50:36.675516Z","shell.execute_reply":"2024-12-30T22:50:43.789739Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"('potato_wizard_v59/tokenizer_config.json',\n 'potato_wizard_v59/special_tokens_map.json',\n 'potato_wizard_v59/tokenizer.json')"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"model.push_to_hub(new_model)\ntokenizer.push_to_hub(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T22:50:57.555175Z","iopub.execute_input":"2024-12-30T22:50:57.555848Z","iopub.status.idle":"2024-12-30T22:52:02.206642Z","shell.execute_reply.started":"2024-12-30T22:50:57.555814Z","shell.execute_reply":"2024-12-30T22:52:02.205774Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.47G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d4393e27a49490d8d166dbcdf189833"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc74a1279dbd43bcb7994f456f63c335"}},"metadata":{}},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/ShadrackImai/potato_wizard_v59/commit/0805f06754800cbf1ab14e9b7b5fcd03e1a73e10', commit_message='Upload tokenizer', commit_description='', oid='0805f06754800cbf1ab14e9b7b5fcd03e1a73e10', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ShadrackImai/potato_wizard_v59', endpoint='https://huggingface.co', repo_type='model', repo_id='ShadrackImai/potato_wizard_v59'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}