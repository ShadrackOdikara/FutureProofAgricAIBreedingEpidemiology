{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceType":"datasetVersion","sourceId":10333286,"datasetId":5971659,"databundleVersionId":10640681}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nimport os\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\n#hf_token = user_secrets.get_secret(\"HUGGINGFACEHUB_API_TOKEN\")\n#login(token=hf_token)\n\nos.environ['HUGGINGFACEHUB_API_TOKEN']='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'\n\nwb_token = user_secrets.get_secret(\"wandb_scoring_mpnet\")\nwandb.login(key=wb_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T12:25:49.484538Z","iopub.execute_input":"2025-01-09T12:25:49.484833Z","iopub.status.idle":"2025-01-09T12:25:59.662567Z","shell.execute_reply.started":"2025-01-09T12:25:49.484794Z","shell.execute_reply":"2025-01-09T12:25:59.661357Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshadrack-imai\u001b[0m (\u001b[33mshadrack-imai-kenya-agricultural-and-livestock-research-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install necessary library\n!pip install -U sentence-transformers\n\n\"\"\"\nThis notebook trains and evaluates a SentenceTransformer model, fine-tuned\nfor sequence classification. The model scores semantic similarities between\nuser queries and responses, aligned with SDG targets.\n\"\"\"\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport torch\nimport math\nfrom datasets import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import SentenceTransformer, InputExample, losses\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Define dataset paths\nReportSDGDatapath = '/kaggle/input/futproofagricaireportsvssdgtargets/curated_agric_dataset_v3.csv'\nInputResponseDatapath = '/kaggle/input/futproofagricaireportsvssdgtargets/QA_data.csv'\nPotatoDiseaseDatapath = '/kaggle/input/futproofagricaireportsvssdgtargets/potatoDiseaseprofile.csv'\n\n# Load datasets\nReportSDGData = pd.read_csv(ReportSDGDatapath)\nInputResponseData = pd.read_csv(InputResponseDatapath)\nPotatoDiseaseData = pd.read_csv(PotatoDiseaseDatapath)\n\n# Data preparation: Adjust column names and replace labels\nInputResponseData.rename(columns={'response': 'premise', 'input': 'hypothesis'}, inplace=True)\nReportSDGData['label'] = ReportSDGData['label'].astype(int)  # Ensure numeric labels\n\n# Drop invalid rows from dataset\nrows_to_drop = list(range(5394, 9284)) + list(range(9662, 10211)) + list(range(10581, 10898)) + list(range(11146, 11261)) + list(range(12307, 13635))\nReportSDGData = ReportSDGData.drop(rows_to_drop)\n\n# Split dataset into training and testing sets\ntrain_df, test_df = train_test_split(ReportSDGData, test_size=0.2, random_state=42)\n\n# Convert datasets to Hugging Face Dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Convert datasets to InputExample format\ntrain_examples = [\n    InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]], label=row[\"label\"])\n    for _, row in train_df.iterrows()\n]\ntest_examples = [\n    InputExample(texts=[row[\"premise\"], row[\"hypothesis\"]], label=row[\"label\"])\n    for _, row in test_df.iterrows()\n]\n\n# Clean data to ensure compatibility\nfor idx, example in enumerate(train_examples):\n    if any(not isinstance(text, str) for text in example.texts):\n        print(f\"Issue at index {idx} in training data: {example.texts}\")\n        example.texts = [str(text) if not isinstance(text, str) else text for text in example.texts]\n\nfor idx, example in enumerate(test_examples):\n    if any(not isinstance(text, str) for text in example.texts):\n        print(f\"Issue at index {idx} in testing data: {example.texts}\")\n        example.texts = [str(text) if not isinstance(text, str) else text for text in example.texts]\n\n# Create DataLoader for training\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=4)\n\n# Load SentenceTransformer model\nmodel = SentenceTransformer(\"all-mpnet-base-v2\", device=device)\n\n# Define loss function for multi-class classification\ntrain_loss = losses.SoftmaxLoss(\n    model=model,\n    sentence_embedding_dimension=model.get_sentence_embedding_dimension(),\n    num_labels=3  # Number of classes: 'neutral', 'entailment', 'contradiction'\n)\n\n# Define evaluator\n#evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_examples, name=\"test-eval\")\nevaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_examples, name=\"test-eval\", similarity_fn_names=[\"cosine\", \"euclidean\", \"manhattan\", \"dot\"]) \n\n# Training configuration\nnum_epochs = 3\nwarmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% warmup\n\n# Train the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    evaluator=evaluator,\n    epochs=num_epochs,\n    warmup_steps=warmup_steps,\n    output_path=\"fine_tuned_model\",\n    evaluation_steps=100  # Evaluate every 100 steps\n)\n\n# Ensure output directory exists\noutput_dir = \"test_results\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Evaluate on the test dataset\nevaluator(model, output_path=output_dir)\nprint(\"Test results saved successfully.\")\n\n# Save the fine-tuned model\n#model.save(\"fine_tuned_model\")\n#print(\"Fine-tuned model saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T01:04:45.279320Z","iopub.execute_input":"2024-12-27T01:04:45.279727Z","iopub.status.idle":"2024-12-27T01:05:11.026710Z","shell.execute_reply.started":"2024-12-27T01:04:45.279702Z","shell.execute_reply":"2024-12-27T01:05:11.025313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"all-mpnet-base-v2\")\nmodel.save_pretrained(\"fine_tuned_scoring_model\")\ntokenizer.save_pretrained(\"fine_tuned_scoring_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-27T01:05:11.027654Z","iopub.status.idle":"2024-12-27T01:05:11.028060Z","shell.execute_reply":"2024-12-27T01:05:11.027919Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"!pip install sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-12-25T05:43:21.692762Z","iopub.execute_input":"2024-12-25T05:43:21.693143Z","iopub.status.idle":"2024-12-25T05:43:24.780664Z","shell.execute_reply.started":"2024-12-25T05:43:21.693112Z","shell.execute_reply":"2024-12-25T05:43:24.779576Z"}}},{"cell_type":"markdown","source":"\"\"\"This notebook is design to train and evaluate an AI based on MPNet architecture and fine tuned\n for sequnce classifcation. We train sentence tranformer for sequnce classification\nfor scoring the semantic similarities for llm responce to user query to the SDG they address\nand whether the confidence scores are high or low to meassure the \"\"\"\nimport numpy as np\nimport pandas as pd\nimport os\nimport torch\n\nfor dirname, _, filenames in os.walk('/kaggle/input/futproofagricaireportsvssdgtargets'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-12-25T06:42:58.333007Z","iopub.execute_input":"2024-12-25T06:42:58.333334Z","iopub.status.idle":"2024-12-25T06:42:58.347257Z","shell.execute_reply.started":"2024-12-25T06:42:58.333310Z","shell.execute_reply":"2024-12-25T06:42:58.346524Z"}}},{"cell_type":"markdown","source":"# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:43:09.746433Z","iopub.execute_input":"2024-12-25T06:43:09.746759Z","iopub.status.idle":"2024-12-25T06:43:09.750675Z","shell.execute_reply.started":"2024-12-25T06:43:09.746731Z","shell.execute_reply":"2024-12-25T06:43:09.749765Z"}}},{"cell_type":"markdown","source":"# We set up out huggingface key and embed it to the notebook\nos.environ['HUGGINGFACEHUB_API_TOKEN']='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'","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:43:14.865093Z","iopub.execute_input":"2024-12-25T06:43:14.865435Z","iopub.status.idle":"2024-12-25T06:43:14.868815Z","shell.execute_reply.started":"2024-12-25T06:43:14.865401Z","shell.execute_reply":"2024-12-25T06:43:14.867798Z"}}},{"cell_type":"markdown","source":"# we store our path to where our dataset is stored on kaggle in ReportSDGDatapath\n# From this path we will be able to load our dataset to the environment since they \n# are linked.\n\nReportSDGDatapath = '/kaggle/input/futproofagricaireportsvssdgtargets/curated_agric_dataset_v3.csv'\nInputResponseDatapath = '/kaggle/input/futproofagricaireportsvssdgtargets/QA_data.csv'\nPotatoDiseaseDatapath = '/kaggle/input/futproofagricaireportsvssdgtargets/potatoDiseaseprofile.csv'","metadata":{"execution":{"iopub.status.busy":"2024-12-25T06:43:30.524843Z","iopub.execute_input":"2024-12-25T06:43:30.525131Z","iopub.status.idle":"2024-12-25T06:43:30.528736Z","shell.execute_reply.started":"2024-12-25T06:43:30.525109Z","shell.execute_reply":"2024-12-25T06:43:30.527867Z"}}},{"cell_type":"markdown","source":"\n# We adjust and edit our dataset in the Dataframe object by specifying the labels \n# and converting the labled into string values so that it can be processed smulteneuosly with \n# the other columns of the datase. We have three columns so far 'premise', 'hypothesis', and 'label'\nReportSDGData['label'] = ReportSDGData['label'].replace({'0': 'neutral', '1': 'entailment', '2': 'contradiction'})\nInputResponseData.rename(columns = {'response': 'premise', 'input': 'hypothesis'}, inplace=True)\n\nInputResponseData = InputResponseData[['premise', 'hypothesis']]\nReportSDGData# We load our dataset which is in the path stored in ReportSDGDatapath using\n# pandas  read_csv. This is stored as a dataframe in object ReportSDGData\nReportSDGData = pd.read_csv(ReportSDGDatapath)\nInputResponseData = pd.read_csv(InputResponseDatapath)\nPotatoDiseaseData = pd.read_csv(PotatoDiseaseDatapath)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-25T07:18:45.571743Z","iopub.execute_input":"2024-12-25T07:18:45.572035Z","iopub.status.idle":"2024-12-25T07:18:45.953864Z","shell.execute_reply.started":"2024-12-25T07:18:45.572014Z","shell.execute_reply":"2024-12-25T07:18:45.952910Z"}}},{"cell_type":"markdown","source":"from sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nfrom torch.utils.data import DataLoader\nfrom sentence_transformers import SentenceTransformer, InputExample\n\n# Convert pandas DataFrame to Dataset\n#dataset = Dataset.from_pandas(ReportSDGData)\n\n# Load the model\nmodel = SentenceTransformer(\"all-mpnet-base-v2\")\n# 2. Fine-Tuning the Similarity Scoring Model\n\n# Split the pandas DataFrame\ntrain_df, test_df = train_test_split(ReportSDGData, test_size=0.2, random_state=42)\n\n# Convert the splits into Hugging Face Dataset format\ntrain_dataset = Dataset.from_pandas(train_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Convert train dataset to InputExample format\ntrain_examples = [\n    InputExample(\n        texts=[row[\"premise\"], row[\"hypothesis\"]],\n        label=float(row[\"label\"])\n    )\n    for row in train_dataset\n]\n\n# Convert test dataset to InputExample format (optional)\ntest_examples = [\n    InputExample(\n        texts=[row[\"premise\"], row[\"hypothesis\"]],\n        label=float(row[\"label\"])\n    )\n    for row in test_dataset\n]\n\n# Create a DataLoader for training\ntrain_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-25T07:32:31.470490Z","iopub.execute_input":"2024-12-25T07:32:31.470812Z","iopub.status.idle":"2024-12-25T07:32:33.620184Z","shell.execute_reply.started":"2024-12-25T07:32:31.470788Z","shell.execute_reply":"2024-12-25T07:32:33.619431Z"}}},{"cell_type":"markdown","source":"from sentence_transformers import losses\n\n# Cosine similarity loss for binary classification\n#train_loss = losses.CosineSimilarityLoss(model=model)\n\n# Alternatively, for multi-class classification:\ntrain_loss = losses.SoftmaxLoss(\n    model=model, sentence_embedding_dimension=model.get_sentence_embedding_dimension(), num_labels=2\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-25T07:34:21.597997Z","iopub.execute_input":"2024-12-25T07:34:21.598353Z","iopub.status.idle":"2024-12-25T07:34:21.603323Z","shell.execute_reply.started":"2024-12-25T07:34:21.598323Z","shell.execute_reply":"2024-12-25T07:34:21.602475Z"}}},{"cell_type":"markdown","source":"from sentence_transformers import SentenceTransformer, losses\nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\nimport math\n\n# Define evaluator\nevaluator = EmbeddingSimilarityEvaluator.from_input_examples(train_examples, name=\"train-eval\")\n\n# Configure training\nnum_epochs = 4\nwarmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1)  # 10% warmup steps\n\n# Train the model\nmodel.fit(\n    train_objectives=[(train_dataloader, train_loss)],\n    evaluator=evaluator,\n    epochs=num_epochs,\n    warmup_steps=warmup_steps,\n    output_path=\"fine_tuned_model\"\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-12-25T07:34:51.480095Z","iopub.execute_input":"2024-12-25T07:34:51.480463Z"}}},{"cell_type":"markdown","source":"# Load test dataset\ntest_examples = [\n    InputExample(\n        texts=[row[\"premise\"], row[\"hypothesis\"]],\n        label=float(row[\"label\"])\n    )\n    for row in dataset[\"test\"]\n]\n\ntest_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_examples, name=\"test-eval\")\ntest_evaluator(model, output_path=\"test_results\")\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}